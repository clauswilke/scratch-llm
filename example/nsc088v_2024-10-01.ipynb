{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.llm import LLM\n",
    "from model.tokenizer import Tokenizer, train_tokenizer\n",
    "\n",
    "from helpers.dataset import NextTokenPredictionDataset\n",
    "from helpers.trainer import train\n",
    "from helpers.config import LLMConfig, TrainingConfig, get_device\n",
    "\n",
    "print(f\"pytorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = LLMConfig(\n",
    "    vocab_size = 4096,\n",
    "    seq_len = 128,\n",
    "    dim_emb = 256,\n",
    "    num_layers = 4,\n",
    "    num_heads = 8,\n",
    "    emb_dropout = 0.0,\n",
    "    ffn_dim_hidden = 4 * 256,\n",
    "    ffn_bias = False\n",
    ")\n",
    "train_config = TrainingConfig(\n",
    "    retrain_tokenizer = False,\n",
    "    device = get_device(),\n",
    "    batch_size = 64,\n",
    "    learning_rate = 3e-4,\n",
    "    weight_decay = 1e-5,\n",
    "    max_epochs = 1,\n",
    "    log_frequency = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../data/tinyshakespeare.txt\"\n",
    "output_file = Path(input_file).with_suffix(\".model\")\n",
    "\n",
    "if not output_file.exists() or train_config.retrain_tokenizer:\n",
    "    train_tokenizer(input_file, llm_config.vocab_size)\n",
    "\n",
    "tokenizer = Tokenizer(str(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcd440",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Before we proceed any further, hear me speak.\"\n",
    "print(tokenizer.sp.EncodeAsPieces(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = NextTokenPredictionDataset(input_file, llm_config.seq_len, tokenizer)\n",
    "dl_train = DataLoader(ds_train, batch_size=train_config.batch_size, shuffle=True)\n",
    "##\n",
    "for inputs, labels in dl_train:\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break\n",
    "\n",
    "device = get_device()\n",
    "model = LLM(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    seq_len = llm_config.seq_len,\n",
    "    dim_emb = llm_config.dim_emb,\n",
    "    num_layers = llm_config.num_layers,\n",
    "    attn_num_heads = llm_config.num_heads,\n",
    "    emb_dropout = llm_config.emb_dropout,\n",
    "    ffn_hidden_dim = llm_config.ffn_dim_hidden,\n",
    "    ffn_bias = llm_config.ffn_bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = train(\n",
    "#     model,\n",
    "#     dl_train,\n",
    "#     train_config.device,\n",
    "#     lr = train_config.learning_rate,\n",
    "#     max_epochs = train_config.max_epochs,\n",
    "#     weight_decay = train_config.weight_decay,\n",
    "#     log_every = train_config.log_frequency\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bcf2e5",
   "metadata": {},
   "source": [
    "Un-comment block above to train model (takes a few minutes on the AMD GPU cluster) **or** just use pre-trained model using block below (must upload this to your home directory for this work):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc92994",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"shakespeare_llm.pt\",\n",
    "                                 weights_only = True,\n",
    "                                 map_location = device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty prompt to generate random stuff\n",
    "prompt = torch.full((1, llm_config.seq_len), tokenizer.eos_id, dtype=torch.int32)\n",
    "prompt = prompt.to(train_config.device)\n",
    "out = model.generate(prompt, max_seq_len=64, top_p=1)\n",
    "tokenizer.decode(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from a prompt\n",
    "prompt = tokenizer.encode(\n",
    "    \"KING HENRY VI:\",\n",
    "    beg_of_string = True,\n",
    "    pad_seq = True,\n",
    "    seq_len = llm_config.seq_len\n",
    ")\n",
    "inputs = torch.tensor(prompt, dtype=torch.int32).unsqueeze(0).to(train_config.device)\n",
    "out = model.generate(inputs, max_seq_len=64, top_p=1)\n",
    "tokenizer.decode(out.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "parList = list(model.parameters())\n",
    "len(parList) ## 35\n",
    "parShapes = [list(el.shape) for el in parList]\n",
    "parShapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77deb70a",
   "metadata": {},
   "source": [
    "parameters consist of:\n",
    "\n",
    "- **0** :: weight matrix for **token embeddings**\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- **1** :: **RMSNorm** parameter vector\n",
    "- **2** :: Q, K, V matrices (concatenated) for **MultiHeadAttention**\n",
    "- **3** :: weight matrix for projout part of **MultiHeadAttention**\n",
    "- **4** :: **RMSNorm** parameter vector\n",
    "- **5** :: initial weight matrix for **FeedForward (SwiGLU)** part\n",
    "- **6** :: **SwiGLU** weight matrices (concatened)\n",
    "- **7** :: **SwiGLU** bias vector\n",
    "- **8** :: final weight matrix for **FeedForward (SwiGLU)** part\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-  **9-16** :: as 1-8 but for second TransformerBlock\n",
    "- **17-24** ::        \"       third         \"\n",
    "- **25-32** ::        \"       fourth        \"\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "- **33** :: **RMSNorm** parameter vector\n",
    "- **34** :: final **projection_head** bias vector\n",
    "\n",
    "**NOTE**: there is no weight matrix for the final projection head b/c it is \"weight-tied\" to the token embeddings weight matrix (0 above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tokens = np.array([tokenizer.sp.id_to_piece(i)\n",
    "                   for i in range(llm_config.vocab_size)])\n",
    "\n",
    "tokenizer.sp.piece_to_id(\"▁perforce\")\n",
    "tokenizer.sp.piece_to_id(\"▁basilisk\")\n",
    "\n",
    "embeddings = pd.DataFrame(parList[0].cpu().detach(), index=tokens)\n",
    "embeddings.loc[\"▁basilisk\"]\n",
    "# embeddings.loc[\"▁perforce\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
